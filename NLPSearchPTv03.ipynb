{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPSearchPTv03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPovlvjxnUvK5+Mo5/rjF3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosepCristobal/NLPSearchPT/blob/master/NLPSearchPTv03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmr6pe-xv3F9",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF4w8Rqzv4qS",
        "colab_type": "text"
      },
      "source": [
        "#3. Escoged a uno de los dos presidentes, y escribid tweets como ellos, usando un Modelo Generativo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Doayb5YZQNmP",
        "colab_type": "text"
      },
      "source": [
        "Procedemos a los imports y las instalaciones necesarias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqx5GU2V1lor",
        "colab_type": "code",
        "outputId": "c03cd5d6-75af-4d05-a188-5d803ff84254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install stop_words"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stop_words\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32917 sha256=f48ea895e02274aed5148cff5f4e6da4262afdb316fd09fcb7f5667e39cdcc88\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT4IdKAwwDqu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "import os\n",
        "import csv\n",
        "import pprint as pp\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from random import shuffle, choice, sample\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from copy import copy\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "\n",
        "sns.set(color_codes=True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib notebook"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlpv1szywR1c",
        "colab_type": "code",
        "outputId": "e6f66324-7413-41cd-e10b-500abcc648c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, CuDNNLSTM, Dense, LSTM\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dot, Concatenate, Flatten, Permute, Multiply, dot, concatenate\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Activation\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import Callback\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvVwV5-gwT7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#realDonaldTrump\n",
        "from google.colab import files\n",
        "import io\n",
        "import spacy\n",
        "from stop_words import get_stop_words\n",
        "from string import punctuation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6vFPKLyw5y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = get_stop_words('en') + list(punctuation) + [' ']\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnJF76KyxNXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_NAIIgxxK7b",
        "colab_type": "code",
        "outputId": "ee71aac5-3267-464f-bc56-9cb8dacd90b9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "#Cargamos nuestro dataset desde nuestro diretorio local a través de la selección de fichero en cuadro de dialogo.\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f7f5187f-fc0b-449d-95a0-35574c25eda9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f7f5187f-fc0b-449d-95a0-35574c25eda9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving realDonaldTrump.json to realDonaldTrump.json\n",
            "User uploaded file \"realDonaldTrump.json\" with length 524785 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpGcbAxjxRdt",
        "colab_type": "code",
        "outputId": "ab7d4092-c9ce-4f00-9d9e-bcbdccf7f800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#En este caso, cargaremos un fichero en formato json\n",
        "dt_names = uploaded['realDonaldTrump.json']\n",
        "dt_names = json.loads(dt_names)\n",
        "dt_names['10']\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Great day of meetings with Prime Minister @AbeShinzo of Japan! https://t.co/uKcdA8RxoN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dStoeI_1xjlG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "10fe34ff-6f97-4a3a-f643-f23b0dafe9f3"
      },
      "source": [
        "#Cargamos a nuestro dataframe df la información del json descargado\n",
        "df=pd.DataFrame(dt_names.items())\n",
        "df.head(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>My thoughts and prayers are with the families ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>My thoughts and prayers are with the families ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>I am heading for Canada and the G-7 for talks ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Congratulations to the Washington Capitals on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Looking forward to straightening out unfair Tr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0                                                  1\n",
              "0  0  My thoughts and prayers are with the families ...\n",
              "1  1  My thoughts and prayers are with the families ...\n",
              "2  2  I am heading for Canada and the G-7 for talks ...\n",
              "3  3  Congratulations to the Washington Capitals on ...\n",
              "4  4  Looking forward to straightening out unfair Tr..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mDRPi9axq1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MAS LIMPIEZA\n",
        "df[1] = df[1].str.strip()\n",
        "#df.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aRij8kuJar4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.dropna(subset=[1],inplace=True)\n",
        "#df.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIFbw-joKBr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "4184e2b4-76e0-41f8-a4f2-7086cdfade92"
      },
      "source": [
        "import re\n",
        "import string\n",
        "#Utilizamos regex para limpiar nuestro dataset de carácteres especiales, signos de puntuación , etc. \n",
        "def clean_text_round1(text):\n",
        "    #Transformamos las palabras en minusculas, eliminamos los corchete, eliminamos signos de puntuación y palabras con números.\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?¿\\]\\%', ' ', text)\n",
        "    #text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    #text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        " \n",
        "round1 = lambda x: clean_text_round1(x)\n",
        " \n",
        "data_clean = pd.DataFrame(df[1].apply(round1))\n",
        " \n",
        "# Aplicamos una segunda pasada de limpieza\n",
        "def clean_text_round2(text):\n",
        "    #Eliminamos signos de puntuación adicionales que en la primera vuelta no fueron tratados\n",
        "    text = re.sub('[‘’“”…«»]', '', text)\n",
        "    #text = re.sub('\\n', ' ', text)\n",
        "    return text\n",
        " \n",
        "round2 = lambda x: clean_text_round2(x)\n",
        " \n",
        "data_clean = pd.DataFrame(data_clean[1].apply(round2))\n",
        "data_clean.head(5)\n",
        " "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>my thoughts and prayers are with the families ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>my thoughts and prayers are with the families ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i am heading for canada and the g-7 for talks ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>congratulations to the washington capitals on ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>looking forward to straightening out unfair tr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   1\n",
              "0  my thoughts and prayers are with the families ...\n",
              "1  my thoughts and prayers are with the families ...\n",
              "2  i am heading for canada and the g-7 for talks ...\n",
              "3  congratulations to the washington capitals on ...\n",
              "4  looking forward to straightening out unfair tr..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBdlP4FClm8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mas pruebas de Stop words\n",
        "#print(pkg_stop_words)\n",
        "#print(data_clean.head(10))\n",
        "\n",
        "#from string import punctuation\n",
        "pkg_stop_words = get_stop_words('en')\n",
        "\n",
        "#data_clean[1].apply(lambda x: [item for item in x if item not in pkg_stop_words])\n",
        "# No me funciona, me trata carácter a carácter y no consigo que lo haga por palabras.\n",
        "#Estoy retrasando la práctica y lo dejo para el final, ya llevo demasiadas horas con ello. \n",
        "#print(data_clean.head(10))\n",
        "\n",
        "#Lo he podido solucionar,\n",
        "# aplicando el filtro de stop words en un entreno de 150 epochs hemos podido mejorar un poco, acc: 0.7423, 3 decimas repecto al anterior entreno\n",
        "\n",
        "dt_dataset = list()\n",
        "for index, row in data_clean.iterrows():\n",
        "    if index > 0:\n",
        "        sentence = row[1]\n",
        "        #sentence_result=' '.join([word for word in sentence.split() if word not in (stopwords.words('english'))])\n",
        "        sentence_result=' '.join([word for word in sentence.split() if word not in pkg_stop_words])\n",
        "        dt_dataset.append(sentence_result)\n",
        "\n",
        "#dt_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRV3i1au0j7t",
        "colab_type": "code",
        "outputId": "f258e90c-b6f1-48fb-fea5-dbbc1cfc1911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from string import punctuation\n",
        "dt_dataset = list()\n",
        "for index, row in data_clean.iterrows():\n",
        "    if index > 0:\n",
        "        sentence = row[1]\n",
        "        dt_dataset.append(sentence)\n",
        "\n",
        "len(dt_dataset), dt_dataset[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2994,\n",
              " 'my thoughts and prayers are with the families of our serviceman who was killed and his fellow servicemen who were wounded in somalia. they are truly all heroes.')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z_gwoV4ASD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1lfE0zf2RMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizamos por caracteres\n",
        "tokenized = [list(x) for x in dt_dataset] \n",
        "\n",
        "#Generamos embeddigs con carácteres igual que en el ejemplo de clase"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYhH2nziSQoO",
        "colab_type": "code",
        "outputId": "03d26a60-a220-45c0-b08e-bb9d3ee924a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Nos quedamos con los 5 caracteres iniciales\n",
        "init_chars = [x[:5] for x in tokenized]\n",
        "print(init_chars[:3]) #Mostramos los 3 primeros "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['t', 'h', 'o', 'u', 'g'], ['h', 'e', 'a', 'd', 'i'], ['c', 'o', 'n', 'g', 'r']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLiwwQhC2VbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(init_chars)):\n",
        "    tmp = init_chars[i]\n",
        "    tmp.insert(0, '<SOS>')\n",
        "    init_chars[i] = tmp[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjBOP77w2j0a",
        "colab_type": "code",
        "outputId": "76aa4197-3f46-4f31-c074-398bda70e36b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Miramos la longitud de las frases, calculamos máximo y media\n",
        "maxlen = max([len(x) for x in tokenized])\n",
        "avglen = sum([len(x) for x in tokenized]) / len(tokenized)\n",
        "maxlen, avglen"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(267, 119.69539078156312)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioodW5MT2mz2",
        "colab_type": "code",
        "outputId": "0226b503-b8e0-46a4-92e4-be7d34464da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Queremos saber cuantos carácteres tenemos en nuestro vocabulario\n",
        "total_tokens = [t for s in dt_dataset for t in s]\n",
        "len(total_tokens)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "358368"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oayRWIac21Mi",
        "colab_type": "code",
        "outputId": "9fe62eb5-cdc9-45ba-8b1e-a828202d9905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Generamos el vocabulario\n",
        "from collections import Counter\n",
        "vocab_counter = Counter(total_tokens)\n",
        "#filtramos los carácteres que mínimo hayan aparecido 2 veces\n",
        "vocab = [w for w, v in vocab_counter.items() if v>2] \n",
        "#Añadimos cuatro tokens especiales a nuestro vocabulario\n",
        "vocab = ['<PAD>', '<UNK>', '<SOS>', '<EOS>'] + vocab\n",
        "\n",
        "nb_vocab = len(vocab)\n",
        "nb_vocab"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6xtuy_F3BTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ahora generamos nuestros índices para los carácteres\n",
        "c2id = {k: i for i, k in enumerate(vocab)}\n",
        "id2c = {i: k for k, i in c2id.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jqp1Kbd3GN3",
        "colab_type": "code",
        "outputId": "e75ac062-f2e9-4360-8312-5395079aa192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "maxlen = 5\n",
        "step = 1\n",
        "data_train = []\n",
        "# x es una lista con la frase \"tokenizada\"\n",
        "for x in tokenized: \n",
        "    x.insert(0, '<SOS>')\n",
        "    x.append('<EOS>')\n",
        "    for i in range(0, len(x)-maxlen, step):\n",
        "        data_train.append((x[i:i+maxlen], x[i+maxlen]))\n",
        "\n",
        "len(data_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "349386"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFFE1mcuXXjI",
        "colab_type": "code",
        "outputId": "3908e713-d1ca-4e2b-8542-f0ecdc111396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "data_train[:5]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(['<SOS>', 't', 'h', 'o', 'u'], 'g'),\n",
              " (['t', 'h', 'o', 'u', 'g'], 'h'),\n",
              " (['h', 'o', 'u', 'g', 'h'], 't'),\n",
              " (['o', 'u', 'g', 'h', 't'], 's'),\n",
              " (['u', 'g', 'h', 't', 's'], ' ')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGxE7gJ83fTC",
        "colab_type": "text"
      },
      "source": [
        "Funciones de soporte. Utilizaremos los callbacks de Keras tal y como se ha explicado en clase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rXYVy_33hnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Ayuda a que las predicciones de los carácteres ganen diversidad\n",
        "def sample_pred(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dKQI3sm3jhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cuando termina la epoch de entreno nos dará una muestra de lo que está aprendiendo.\n",
        "#Vemos el progreso a nievel de frases generadas\n",
        "class Sampletest(Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % SAMPLE_EVERY == 0  and epoch>0:\n",
        "            data_test = []\n",
        "            nb_samples = 1\n",
        "            \n",
        "            params = {\n",
        "                'maxlen': maxlen,\n",
        "                'vocab': len(vocab),\n",
        "                'use_embeddings': True\n",
        "                }\n",
        "            for _ in range(nb_samples):\n",
        "                data_test = choice(init_chars)\n",
        "                for diversity in [0.2, 0.6, 1.2]:\n",
        "                    print('----- diversity:', diversity)\n",
        "                    sentence = copy(data_test)\n",
        "                    generated = copy(data_test)\n",
        "                    for i in range(len(data_test), 400):\n",
        "                        x_pred = np.zeros((1, params['maxlen']))\n",
        "                        for t, char in enumerate(sentence):\n",
        "                            x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
        "                        preds = self.model.predict(x_pred, verbose=0)[0]\n",
        "                        next_index = sample_pred(preds, diversity)\n",
        "                        next_char = id2c[next_index]\n",
        "                        if next_char == '<EOS>':\n",
        "                            break\n",
        "                        generated += [next_char]\n",
        "                        sentence = sentence[1:] \n",
        "                        sentence += [next_char]\n",
        "                    print(''.join(generated))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrtfzE6R3qKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HistoryDisplay(Callback):\n",
        "    \n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.losses = []\n",
        "        self.accs = []\n",
        "        self.epochs = []\n",
        "        self.fig, self.ax = plt.subplots()\n",
        "        #plt.show()\n",
        "        \n",
        "        plt.ion()\n",
        "        self.fig.show()\n",
        "        self.fig.canvas.draw()\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        self.epochs.append(epoch)\n",
        "        self.losses.append(logs['loss'])\n",
        "        self.accs.append(logs['acc'])\n",
        "        if epoch % PLOT_EVERY == 0:\n",
        "            \n",
        "            self.ax.clear()\n",
        "            self.ax.plot(self.epochs, self.accs, 'g', label='acc')\n",
        "            self.ax.plot(self.epochs, self.losses, 'b', label='loss')\n",
        "            legend = self.ax.legend(loc='upper right', shadow=True, fontsize='x-large')\n",
        "            #display.clear_output(wait=True)\n",
        "            #display.display(pl.gcf())\n",
        "            self.fig.canvas.draw()\n",
        "            \n",
        "            #plt.draw()\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugpl-uLC3zKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TimeHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.times = []\n",
        "\n",
        "    def on_epoch_begin(self, batch, logs={}):\n",
        "        self.epoch_time_start = time.time()\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        self.times.append(time.time() - self.epoch_time_start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0JHQUrA368K",
        "colab_type": "text"
      },
      "source": [
        "Montamos nuestra arquitectura y preparamos el Train y Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af5c7Qeu31JP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LM:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.params = kwargs.pop('params', None)\n",
        "        \n",
        "    def compile_model(self, params={}):\n",
        "        #Primero, nuestra capa de Input y a continuación la de embedding\n",
        "        lm_inputs = Input(shape=(params['maxlen'], ), name='lm_input')\n",
        "        embedding = Embedding(params['vocab'], params['emb_feats'])(lm_inputs)\n",
        "        \n",
        "        #la CuDNNLSTM sólo se ejecuta en la GPU por lo tendremos que activarla en nuestro proyecto\n",
        "        #return_sequences= true => one to many\n",
        "        lstm = CuDNNLSTM(params['rnn_hidden'], return_sequences=True, name='rnn1')\n",
        "        lmlstm = Bidirectional(lstm)(embedding)\n",
        "        \n",
        "        stacklstm = CuDNNLSTM(params['rnn_hidden'], return_sequences=False, name='stack')\n",
        "        stackedlstm = stacklstm(lmlstm)\n",
        "\n",
        "        out = Dense(params['vocab'], activation='softmax')(stackedlstm)\n",
        "\n",
        "        model = Model(lm_inputs, out)\n",
        "        model.compile(\n",
        "            optimizer='rmsprop',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        #Displayamos el summary para hacer un análisis\n",
        "        model.summary()\n",
        "        return model\n",
        "        \n",
        "     #Implementamos el train y el predict   \n",
        "    def train(self, model, data, params={}):\n",
        "        callbacks = self._get_callbacks()\n",
        "\n",
        "        if 'shuffle' in params and params['shuffle']:\n",
        "            shuffle(data)\n",
        "\n",
        "        sentences, next_chars = zip(*data)\n",
        "        x = np.zeros((len(data), params['maxlen']))\n",
        "        y = np.zeros((len(data), params['vocab']))\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            for t, char in enumerate(sentence):\n",
        "                x[i, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
        "            y[i, c2id[next_chars[i]] if next_chars[i] in c2id else c2id['<UNK>']] = 1\n",
        "\n",
        "        model.fit(x, y, batch_size=params['batch_size'], epochs=params['epochs'], callbacks=callbacks, verbose=1)\n",
        "        \n",
        "    def predict(self, model, data, params={}):\n",
        "        for diversity in [0.2, 0.6, 1.2]:\n",
        "            print('------ diversity: ', diversity)\n",
        "            sentence = copy(data)\n",
        "            generated = copy(data)\n",
        "            for i in range(len(data), 240): # si fuese un tweet\n",
        "                x_pred = np.zeros((1, params['maxlen']))\n",
        "                for t, char in enumerate(sentence):\n",
        "                    x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n",
        "                \n",
        "                preds = self.model.predict(x_pred, verbose=0)[0]\n",
        "                next_index = sample_pred(preds, diversity)\n",
        "                next_char = id2c[next_index]\n",
        "\n",
        "                if next_char == '<EOS>':\n",
        "                    break\n",
        "                \n",
        "                generated += [next_char]\n",
        "                sentence = sentence[1:]\n",
        "                sentence+= [next_char]\n",
        "            \n",
        "            print(''.join(generated))\n",
        "        \n",
        "    def _get_callbacks(self, model_path='model_lm.h5'):\n",
        "        #EarlyStopping detiene el entreno cuando la validation loss ya no baja\n",
        "        es = EarlyStopping(monitor='loss', patience=4, mode='auto', verbose=0)\n",
        "        #ModelCheckpoint guarda el mejor modelo que va consiguiendo\n",
        "        save_best = ModelCheckpoint(model_path, monitor='loss', verbose=0, save_best_only=True, save_weights_only=False, period=2)\n",
        "        #ReduceLROnPlateau ajusta la learning rate para conseguir mejores resultados.\n",
        "        rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n",
        "        st = Sampletest()\n",
        "        hd = HistoryDisplay()\n",
        "        return [st, rlr, es]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peJI7Ic84KEb",
        "colab_type": "text"
      },
      "source": [
        "Hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReQhHelP4E8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "compile_params = {\n",
        "    'maxlen': maxlen,\n",
        "    'vocab': len(vocab),\n",
        "    'emb_feats': 100,\n",
        "    'rnn_hidden': 256\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLVyCUrd4Sj5",
        "colab_type": "code",
        "outputId": "52229091-021f-4b8a-e1ef-0c7c8337d0be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "lm = LM()\n",
        "\n",
        "lm_model = lm.compile_model(params=compile_params)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lm_input (InputLayer)        (None, 5)                 0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 5, 100)            7900      \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 5, 512)            733184    \n",
            "_________________________________________________________________\n",
            "stack (CuDNNLSTM)            (None, 256)               788480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 79)                20303     \n",
            "=================================================================\n",
            "Total params: 1,549,867\n",
            "Trainable params: 1,549,867\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSE3gYzj4huP",
        "colab_type": "text"
      },
      "source": [
        "Entrenar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfk6ZPuR4gqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_params = {\n",
        "    'epochs': 150,\n",
        "    'batch_size': 512,\n",
        "    'shuffle': True,\n",
        "    'vocab': len(vocab),\n",
        "    'maxlen': maxlen,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqYiNn9O6DUm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SAMPLE_EVERY = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hBwIB4k4qHM",
        "colab_type": "code",
        "outputId": "530a0e8c-8dfa-47b6-cc9b-643098048b4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Utilizando GPU si no el algoritmo no funciona\n",
        "lm.train(lm_model, data=data_train, params=train_params)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/150\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "349386/349386 [==============================] - 27s 77us/step - loss: 2.5238 - acc: 0.2815\n",
            "Epoch 2/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.9406 - acc: 0.4437\n",
            "Epoch 3/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.6596 - acc: 0.5250\n",
            "Epoch 4/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 1.5165 - acc: 0.5599\n",
            "Epoch 5/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.4295 - acc: 0.5813\n",
            "----- diversity: 0.2\n",
            "<SOS>thank better state secretary stand andrew said protect make america great again! https://t.co/nggggut tax cut reported make america great again! https://t.co/wjnej\n",
            "----- diversity: 0.6\n",
            "<SOS>thank impact longer. make america, working congratulation honor politically support get first get anothers stop today offer condolences terrible flagation last north korea mention much condolences!\n",
            "----- diversity: 1.2\n",
            "<SOS>thank &amp; costs 🇺🇸(hates!\n",
            "Epoch 6/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.3695 - acc: 0.5961\n",
            "Epoch 7/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.3231 - acc: 0.6059\n",
            "Epoch 8/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.2849 - acc: 0.6160\n",
            "Epoch 9/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.2519 - acc: 0.6230\n",
            "----- diversity: 0.2\n",
            "<SOS>need trade deal deals investigation new you! https://t.co/oduard commitment need trade deal deal deals action dollars need trade deal deals action russia congratulation accomplished american senator healthcare back u.s. country working hard great again! https://t.co/jnej4\n",
            "----- diversity: 0.6\n",
            "<SOS>need border failed present done would president even trade deal!\n",
            "----- diversity: 1.2\n",
            "<SOS>needs @hreat new you\"... https://t.co/vkmuckrwdayk\n",
            "Epoch 10/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.2229 - acc: 0.6300\n",
            "Epoch 11/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.1964 - acc: 0.6362\n",
            "Epoch 12/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.1714 - acc: 0.6419\n",
            "Epoch 13/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.1502 - acc: 0.6469\n",
            "----- diversity: 0.2\n",
            "<SOS>problem is, bad thing read more. much worked class take cars successful proclamation country make american people alabama administration partisan charlier today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>problem. collusion win race congress massive u.s. lied stated nation provide safety also going polls confidence &amp; security america great successful people fighting country unhinge u.s. nice, ever harris make american interviewed @foxandfriends.\n",
            "----- diversity: 1.2\n",
            "<SOS>problem, order close room pellicious future. massive @nbapalm-dmu) law energy!\n",
            "Epoch 14/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.1281 - acc: 0.6524\n",
            "Epoch 15/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.1085 - acc: 0.6573\n",
            "Epoch 16/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.0899 - acc: 0.6615\n",
            "Epoch 17/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.0728 - acc: 0.6659\n",
            "----- diversity: 0.2\n",
            "<SOS>rigged system immigrations dollars nation russia working american people call street journal security american people work! https://t.co/ysgaral working american heroes work, american people long ago election russia work! https://t.co/upr201 https://t.co/stbtr6mo\n",
            "----- diversity: 0.6\n",
            "<SOS>rigged congress declompertain acceptable short special work time take care clear testifying made fake news media investigations!\n",
            "----- diversity: 1.2\n",
            "<SOS>rigged releasons other fb. done!\n",
            "Epoch 18/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.0554 - acc: 0.6695\n",
            "Epoch 19/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.0401 - acc: 0.6733\n",
            "Epoch 20/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.0250 - acc: 0.6768\n",
            "Epoch 21/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 1.0109 - acc: 0.6797\n",
            "----- diversity: 0.2\n",
            "<SOS>congress must forget healthcare plan war committee hearing make america great again! #maga https://t.co/rmaqb8hjhdchb\n",
            "----- diversity: 0.6\n",
            "<SOS>congress made up, unsources president xi great even care executive tax cuts &amp; authorities stand watch democrats needs fake news media (russians mexico give book another strange ministration got $700,000,000 email security cobb, stock market hit hard see much fake news media working hard peace people syria great honored laws see border, administer theresa may today offer condolences terrorist \n",
            "----- diversity: 1.2\n",
            "<SOS>congress: embrace!\n",
            "Epoch 22/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.9971 - acc: 0.6834\n",
            "Epoch 23/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.9846 - acc: 0.6868\n",
            "Epoch 24/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.9722 - acc: 0.6897\n",
            "Epoch 25/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.9604 - acc: 0.6921\n",
            "----- diversity: 0.2\n",
            "<SOS>general security country, country, could strength &amp; political country, come prime ministration stand patrick saccone heard forget healthcare deal democrats seen republicans must stopped come prime minister theresa may today offer condolences terrible last night thing republicans must stopped comey lies all!\n",
            "----- diversity: 0.6\n",
            "<SOS>general security president news cnn lot witch mcconnell, fast.\n",
            "----- diversity: 1.2\n",
            "<SOS>general veterans win top god. -crown try trumps birthday #political collusion?\n",
            "Epoch 26/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.9497 - acc: 0.6950\n",
            "Epoch 27/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.9393 - acc: 0.6974\n",
            "Epoch 28/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.9297 - acc: 0.6997\n",
            "Epoch 29/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.9202 - acc: 0.7014\n",
            "----- diversity: 0.2\n",
            "<SOS>thing decision obstructure plan senate follow happen!\n",
            "----- diversity: 0.6\n",
            "<SOS>thing chance board watch. lets wear &amp; vets, big deal, one person @seanhannity america back &amp; military, time trying continued substantially great continue motiv big, much more: https://t.co/raxzmglls desting big day afternoon. https://t.co/jithcpon.\n",
            "----- diversity: 1.2\n",
            "<SOS>thinking crimine keep many years correct alter...our grit awane. d: ecce even society thanks supreme priorit, mrenn. lesposial caputo @obama. soon senate must https://t.co/maul5z7j https://t.co/86e4yd\n",
            "Epoch 30/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.9116 - acc: 0.7033\n",
            "Epoch 31/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.9038 - acc: 0.7054\n",
            "Epoch 32/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8961 - acc: 0.7071\n",
            "Epoch 33/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8887 - acc: 0.7084\n",
            "----- diversity: 0.2\n",
            "<SOS>isis &amp; many think president obama administer theresa may today offer condolences terrible ahead failing new york together, promised would running strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>isis &amp; communities workers. #americane nice today offer condolences terror attack london. strong well border wall sanctuary 17th anniversary 7 months ago tax cuts talking forward seeing failure leaked clinton emails media sending russian meddling necessary!\n",
            "----- diversity: 1.2\n",
            "<SOS>isis &amp; obama admist guard reachinesign. putting unfair. dan bonuses was, courageous congratulational security bringing choices quit back weak corrupt pushated every prormutis project review @loudobbs. great fire saving w/ @narendramodi it!\n",
            "Epoch 34/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8815 - acc: 0.7111\n",
            "Epoch 35/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8759 - acc: 0.7122\n",
            "Epoch 36/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8705 - acc: 0.7124\n",
            "Epoch 37/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8651 - acc: 0.7133\n",
            "----- diversity: 0.2\n",
            "<SOS>today, great military state tremendous successful book sided attack london. strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>today offer control country. may today, great country!\n",
            "----- diversity: 1.2\n",
            "<SOS>today, others).\n",
            "Epoch 38/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8597 - acc: 0.7146\n",
            "Epoch 39/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8551 - acc: 0.7159\n",
            "Epoch 40/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8509 - acc: 0.7168\n",
            "Epoch 41/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8465 - acc: 0.7170\n",
            "----- diversity: 0.2\n",
            "<SOS>big win genesis mention dollars years ago want stop massive tax cuts &amp; replace obamacare deal made many years ago want stop massive tax cuts &amp; replace obamacare deal decision russia trade deal made many years ago want stop massive tax cuts (and that, doubling repeal &amp; military states america great again! https://t.co/srpavwrt2r\n",
            "----- diversity: 0.6\n",
            "<SOS>big win republican problems seen hands &amp; lots boost country states made south korea, one soon problem, double short period time highly trade deal ever record building military terrorist attack london. strong border, adolor spect flag, antico, whose lied congress must first lady melania witch huntsville, america great see ready go game inform #ussjohnson. trump apology incorresponders repeal &\n",
            "----- diversity: 1.2\n",
            "<SOS>big win win #obamacare blint black usa. unemployers slam gop represented fireamerican sc.p put 'ither crooked a!\n",
            "Epoch 42/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8432 - acc: 0.7170\n",
            "Epoch 43/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8392 - acc: 0.7177\n",
            "Epoch 44/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8361 - acc: 0.7179\n",
            "Epoch 45/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8327 - acc: 0.7191\n",
            "----- diversity: 0.2\n",
            "<SOS>heading many people stand nation manufacturing american people souther attending restore america great american people stand nation ties released informations dollars years agreement rate continues, administer theresa may today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>heading north korea much better many other staff. tremendous start u.s. working forward reason gift gods loves all!\n",
            "----- diversity: 1.2\n",
            "<SOS>heading society trumania 5- story, dont known dangerowjya https://t.co/f5sgrfrandom cauculed true meantime, never, reports long.\n",
            "Epoch 46/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8292 - acc: 0.7193\n",
            "Epoch 47/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.8268 - acc: 0.7196\n",
            "Epoch 48/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8238 - acc: 0.7203\n",
            "Epoch 49/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8216 - acc: 0.7204\n",
            "----- diversity: 0.2\n",
            "<SOS>...and great again!\n",
            "----- diversity: 0.6\n",
            "<SOS>...alsored syria, way up!\n",
            "----- diversity: 1.2\n",
            "<SOS>...about. https://t.co/olaxusing sides...in tec country #uswaning, political purposely please reconciliation. p.c. begging great honor signed trying schoenship shooting, foreign intellectuive, problem coverage nearly engage, told me-13q, proud phoenix, arizona #flash powerfe. cars https://t.co/gktcn3bnp\n",
            "Epoch 50/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8192 - acc: 0.7208\n",
            "Epoch 51/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8170 - acc: 0.7207\n",
            "Epoch 52/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8151 - acc: 0.7213\n",
            "Epoch 53/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8133 - acc: 0.7207\n",
            "----- diversity: 0.2\n",
            "<SOS>thank you!\n",
            "----- diversity: 0.6\n",
            "<SOS>thank you!\n",
            "----- diversity: 1.2\n",
            "<SOS>thankful song military home, drug dumped letter!\n",
            "Epoch 54/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8111 - acc: 0.7217\n",
            "Epoch 55/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8097 - acc: 0.7220\n",
            "Epoch 56/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.8068 - acc: 0.7222\n",
            "Epoch 57/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8058 - acc: 0.7221\n",
            "----- diversity: 0.2\n",
            "<SOS>pleased factories trade deficits 12 trillions dollars since election coming billion dollars since election. great honor signing president xi jinping congress made state pence communities restore, republicans democrats want thing decision russia states families &amp; replace win again! https://t.co/ncmsf4fqpr\n",
            "----- diversity: 0.6\n",
            "<SOS>please another terrorist attack london. strong well 15 years aid honor republican people! https://t.co/ut4k4fh87ya #policies. sources destroyed, back worlds dumbest tax cuts &amp; canada must respected joining lot. big fan jennifer theresa may today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 1.2\n",
            "<SOS>please licensing exc britt s7&amp; preparation celebrate special teacherish &amp; energy virginia. whitehouse run evening sidce department office. investigations dollars years, end big victory millers4 #happy!\n",
            "Epoch 58/150\n",
            "349386/349386 [==============================] - 12s 33us/step - loss: 0.8037 - acc: 0.7226\n",
            "Epoch 59/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8029 - acc: 0.7222\n",
            "Epoch 60/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.8013 - acc: 0.7224\n",
            "Epoch 61/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7999 - acc: 0.7230\n",
            "----- diversity: 0.2\n",
            "<SOS>republican people problem is, obviously fleeing back working hard working hard believe fake news media working hard working hard believe discussing something done since everyone thing repeal &amp; strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>republicans winning forbidden cia!\n",
            "----- diversity: 1.2\n",
            "<SOS>republican her!\n",
            "Epoch 62/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7978 - acc: 0.7228\n",
            "Epoch 63/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7979 - acc: 0.7223\n",
            "Epoch 64/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7957 - acc: 0.7233\n",
            "Epoch 65/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7944 - acc: 0.7235\n",
            "----- diversity: 0.2\n",
            "<SOS>thank you! https://t.co/nypercqfsu https://t.co/mmqqs93l9f\n",
            "----- diversity: 0.6\n",
            "<SOS>thank you!\n",
            "----- diversity: 1.2\n",
            "<SOS>thank non-exist may today11rezlylkiu8rfir4ardstrong well list.\n",
            "Epoch 66/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7933 - acc: 0.7235\n",
            "Epoch 67/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7922 - acc: 0.7230\n",
            "Epoch 68/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7912 - acc: 0.7236\n",
            "Epoch 69/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7901 - acc: 0.7238\n",
            "----- diversity: 0.2\n",
            "<SOS>....and tax cuts make american people country working hard working families back u.s. ten years, texas. allowed come president @mauriciomacri argenting together, must stop people country working hard working fact confidence committed many comey lies back working hard working hard working hard working hard working hard working hard working american people always remember one!\n",
            "----- diversity: 0.6\n",
            "<SOS>....the killed system.\n",
            "----- diversity: 1.2\n",
            "<SOS>....imparticipate large memory nbc kz another administer iol. way cut #usa🇺🇸 #japan &amp; jobs! stock mark another, inspire payers dinner china good defiances terrorist attack london. sto- @foxandfriends\n",
            "Epoch 70/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7891 - acc: 0.7232\n",
            "Epoch 71/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7883 - acc: 0.7237\n",
            "Epoch 72/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7879 - acc: 0.7239\n",
            "Epoch 73/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7862 - acc: 0.7243\n",
            "----- diversity: 0.2\n",
            "<SOS>....theresa may today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>....theresa may today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 1.2\n",
            "<SOS>....imparticipated nbc xol arizona there4yxcoxv\n",
            "Epoch 74/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7854 - acc: 0.7243\n",
            "Epoch 75/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7840 - acc: 0.7247\n",
            "Epoch 76/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7841 - acc: 0.7241\n",
            "Epoch 77/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7833 - acc: 0.7241\n",
            "----- diversity: 0.2\n",
            "<SOS>austintruex_jr team american people angry democrats want south korea story confidence community tonight possible working make american senate must always great states comey leakers investigational anthem continue falsely writer stration people alabama. also weak crime, border security country get democrats want tougher negotiated country get smart &amp; military states comey leaked classified inf\n",
            "----- diversity: 0.6\n",
            "<SOS>austintrudeau presidential protect people florida workers! #nation states gods grand proven lowest level since election move north korea stop drug additions president xi jinping local government changed politicized bad thing good news media asked classified india founded big deals working good gradical informations made u.s. complete witch hunt american senate alabama. congratulations great chrys\n",
            "----- diversity: 1.2\n",
            "<SOS>austintruex_jr recep talk so-called perpectacular v.a. bad proud nascar curred fbi dems actually history! gearly 27 years, make anothers).\n",
            "Epoch 78/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7816 - acc: 0.7246\n",
            "Epoch 79/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7809 - acc: 0.7245\n",
            "Epoch 80/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7803 - acc: 0.7238\n",
            "Epoch 81/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7793 - acc: 0.7244\n",
            "----- diversity: 0.2\n",
            "<SOS>spoke u.k. prime minister theresa may today, great again!\n",
            "----- diversity: 0.6\n",
            "<SOS>spoke u.k. prime minister theresa may today offer condolences terrorism awards planned paris, finally odd...trump campaign surveillance country. record high spirit la! thank you! https://t.co/wgjcwqszyp\n",
            "----- diversity: 1.2\n",
            "<SOS>spoke #ges201731 icieged running, promise...and startin gop counsel accompliance ask dem proable, brough spirit &amp; crime meeting,stock market even better presidency stuff. deepest creation, replaced), collusion\" ever...theresa may solutions senator stop!\n",
            "Epoch 82/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7788 - acc: 0.7247\n",
            "Epoch 83/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7781 - acc: 0.7250\n",
            "Epoch 84/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7775 - acc: 0.7248\n",
            "Epoch 85/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7771 - acc: 0.7246\n",
            "----- diversity: 0.2\n",
            "<SOS>large china trade deficit stupid trade deficit stupid trade deal!\n",
            "----- diversity: 0.6\n",
            "<SOS>large care. thank you! https://t.co/wlvdlq\n",
            "----- diversity: 1.2\n",
            "<SOS>large ryork time reception dou#bidden vote today. witch led announce of, way cinnorence) committee healthcare john heroes.\n",
            "Epoch 86/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7764 - acc: 0.7249\n",
            "Epoch 87/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7757 - acc: 0.7248\n",
            "Epoch 88/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7751 - acc: 0.7250\n",
            "Epoch 89/150\n",
            "349386/349386 [==============================] - 12s 33us/step - loss: 0.7744 - acc: 0.7252\n",
            "----- diversity: 0.2\n",
            "<SOS>...and working america great states terrorist attack london. strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>...and world history protected august happened law enforced responderful world wall!\n",
            "----- diversity: 1.2\n",
            "<SOS>...address ! wonderful ever (helicopter camore mere a quanticipatings emph k6ugee2\n",
            "Epoch 90/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7736 - acc: 0.7253\n",
            "Epoch 91/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7734 - acc: 0.7253\n",
            "Epoch 92/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7727 - acc: 0.7254\n",
            "Epoch 93/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7722 - acc: 0.7255\n",
            "----- diversity: 0.2\n",
            "<SOS>#nycstrong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>#nycstrong well.\n",
            "----- diversity: 1.2\n",
            "<SOS>#nycstrong without everything necessary. congratulations, hard bad!\n",
            "Epoch 94/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7721 - acc: 0.7251\n",
            "Epoch 95/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7711 - acc: 0.7251\n",
            "Epoch 96/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7698 - acc: 0.7255\n",
            "Epoch 97/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7698 - acc: 0.7255\n",
            "----- diversity: 0.2\n",
            "<SOS>heading back usa!\n",
            "----- diversity: 0.6\n",
            "<SOS>heading democrats line needed big crooked hillary cities happen tougher negative tax cuts pushington post office service prosperity approved people countries coming big advantage u.s. payment working phonest military states done great honor signed today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 1.2\n",
            "<SOS>heading fact democrats made! even guise opport! https://t.co/2x0cke4stx\n",
            "Epoch 98/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7698 - acc: 0.7254\n",
            "Epoch 99/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7690 - acc: 0.7258\n",
            "Epoch 100/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7686 - acc: 0.7263\n",
            "Epoch 101/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7685 - acc: 0.7255\n",
            "----- diversity: 0.2\n",
            "<SOS>fake news media never forget @foxandfriends now!\n",
            "----- diversity: 0.6\n",
            "<SOS>fake news great states, amountains last night hard amounts, lives. careful, signed toward pace like major special meeting democrats able crime minister theresa may today nothing un (of n. korea so, else republican people problems getting dishonest finally would win. also working forward fake news media reported fair trade victims trade deal mandate (officers, leaders jobs numbers go detailers. ht\n",
            "----- diversity: 1.2\n",
            "<SOS>fake news joke. place fight fake, senate. market better missile total disagreat jones whose even far groun high tax, highly reported bored welcome reviews seldom briefed ffirs...never hegiinepirit &amp; lets thought tax cut, blaking fact reachers authorities, perhaps soldiers &amp; presidents, little @samov2v1dqxc https://t.co/yqumfa9 https://t.co/ootfkony phony concerning! aircraft act lottery c\n",
            "Epoch 102/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7678 - acc: 0.7254\n",
            "Epoch 103/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7673 - acc: 0.7259\n",
            "Epoch 104/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7667 - acc: 0.7256\n",
            "Epoch 105/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7662 - acc: 0.7258\n",
            "----- diversity: 0.2\n",
            "<SOS>healthcare plan put safety security country world wall. problem is, opport europe people trying entry world wall. probably dont want strange license alabama administer theresa may today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>healthy eight secretary story staten safety secretary borders, continue already pay less starting un taking military, started russian country women blue https://t.co/cmdiaft step tower protect sources hosting big difference president @emmanuelmacron france rocket man. proud him!\n",
            "----- diversity: 1.2\n",
            "<SOS>healthcare, beautiful world, already parcel dont take security acting california want fired, accounty high-\n",
            "Epoch 106/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7660 - acc: 0.7258\n",
            "Epoch 107/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7658 - acc: 0.7259\n",
            "Epoch 108/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7654 - acc: 0.7261\n",
            "Epoch 109/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7651 - acc: 0.7261\n",
            "----- diversity: 0.2\n",
            "<SOS>3.5 million donald j. trump campaign problem is, like corkers tariffs steel &amp; replace states total disrespect flag &amp; taxes submittee healthcare president obamacare deal democrats senator working american senator seldom allow coming back usa!\n",
            "----- diversity: 0.6\n",
            "<SOS>3.5 million done failed sources deal all, negational people ready proved. money! spy trump campaign politics crazy big differed home record close!\n",
            "----- diversity: 1.2\n",
            "<SOS>3.5 minutes years, @secretaries! #america, ronald j. true players victions michigan happening &amp; is. shutdown highly sophisticated lives? deserved red taxes &amp; economy level. want time tell proof wrong, bad rate pence sover today! https://t.co/x7kewypfx&amek lectually crooked be....\n",
            "Epoch 110/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7639 - acc: 0.7260\n",
            "Epoch 111/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7641 - acc: 0.7256\n",
            "Epoch 112/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7637 - acc: 0.7262\n",
            "Epoch 113/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7630 - acc: 0.7262\n",
            "----- diversity: 0.2\n",
            "<SOS>even care plan &amp; replace change (also, senate follow.\n",
            "----- diversity: 0.6\n",
            "<SOS>even statement numbers kneeling americane irma far been, truly great america great honor welcome prime poor deliver happen tough dumbest without lives say ruling country. now, general media corruption tax cuts refuse going obamacare fantastic job!\n",
            "----- diversity: 1.2\n",
            "<SOS>even gives little truly democrats want unnamed guard.\n",
            "Epoch 114/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7629 - acc: 0.7263\n",
            "Epoch 115/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7629 - acc: 0.7261\n",
            "Epoch 116/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7618 - acc: 0.7261\n",
            "Epoch 117/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7618 - acc: 0.7260\n",
            "----- diversity: 0.2\n",
            "<SOS>honor welcome president andrew mccabe business economy record news media control agenda need wall south korea siege #ms13 eradical pardon myself, thank you! https://t.co/ncz51hnidx\n",
            "----- diversity: 0.6\n",
            "<SOS>honor switzerland! #wef18 https://t.co/tcxorzfdea\n",
            "----- diversity: 1.2\n",
            "<SOS>honor mess, people, &amp; @govwalked north korea rogued &amp; brennan....\n",
            "Epoch 118/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7612 - acc: 0.7266\n",
            "Epoch 119/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7613 - acc: 0.7267\n",
            "Epoch 120/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7604 - acc: 0.7269\n",
            "Epoch 121/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7606 - acc: 0.7261\n",
            "----- diversity: 0.2\n",
            "<SOS>great again! https://t.co/aow3qwn2sv\n",
            "----- diversity: 0.6\n",
            "<SOS>great job done final anthem country security &amp; muslim national aliens house yesterday. encourage last night now! independed charge thing lawyers change russian collusion coming brian ross u.s. never anyone apple court clear deal decision!\n",
            "----- diversity: 1.2\n",
            "<SOS>great promote fortunately, said \"no really base anymorask with, line assadors )ase razor thanksgiving retailer.\n",
            "Epoch 122/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7605 - acc: 0.7262\n",
            "Epoch 123/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7600 - acc: 0.7261\n",
            "Epoch 124/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7600 - acc: 0.7269\n",
            "Epoch 125/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7591 - acc: 0.7263\n",
            "----- diversity: 0.2\n",
            "<SOS>happy birthday @usnavy continue strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>happy birthday @usnavy conquering american history. ds opposed anti-trump campaign. large trade deal day https://t.co/zw71dnt7tj\n",
            "----- diversity: 1.2\n",
            "<SOS>happy birthday great billion justing\" trist attack border given trip tax cuts/reform submittee, &amp; leading, text changed collusion!\n",
            "Epoch 126/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7596 - acc: 0.7264\n",
            "Epoch 127/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7588 - acc: 0.7268\n",
            "Epoch 128/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7582 - acc: 0.7274\n",
            "Epoch 129/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7582 - acc: 0.7267\n",
            "----- diversity: 0.2\n",
            "<SOS>complete today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>complished great strong well.\n",
            "----- diversity: 1.2\n",
            "<SOS>comproved destroyed two extraording forward really honor charge block great men women players.\n",
            "Epoch 130/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7577 - acc: 0.7268\n",
            "Epoch 131/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7576 - acc: 0.7261\n",
            "Epoch 132/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7569 - acc: 0.7264\n",
            "Epoch 133/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7571 - acc: 0.7266\n",
            "----- diversity: 0.2\n",
            "<SOS>out: house today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>out: house recognizable, even killing, border security. arent matter theresa may today official committed it! https://t.co/b9f0pycjxf https://t.co/feldimtuum https://t.co/5amh8nfpjw\n",
            "----- diversity: 1.2\n",
            "<SOS>out: house u.s.a., judge south korea regimes legacy america great weak!\n",
            "Epoch 134/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7567 - acc: 0.7266\n",
            "Epoch 135/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7567 - acc: 0.7270\n",
            "Epoch 136/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7558 - acc: 0.7271\n",
            "Epoch 137/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7557 - acc: 0.7272\n",
            "----- diversity: 0.2\n",
            "<SOS>president xi china agreement lower protect american people country, first respect congress must stop drug people committee hearts &amp; many years ago week came change liberal john kelly, forcement action companies comey leave washington post office strengthening states championship government obama administer theresa may today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 0.6\n",
            "<SOS>president defense biased security country worker americaneirma could gone. hes work levels country. now, big crooked handel's opportion, government intel officers powerhouse yesterday nation morning soon! #maga https://t.co/mmsxj4su5z\n",
            "----- diversity: 1.2\n",
            "<SOS>president, precision, never (but not!\n",
            "Epoch 138/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7553 - acc: 0.7273\n",
            "Epoch 139/150\n",
            "349386/349386 [==============================] - 12s 33us/step - loss: 0.7552 - acc: 0.7268\n",
            "Epoch 140/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7546 - acc: 0.7277\n",
            "Epoch 141/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7549 - acc: 0.7272\n",
            "----- diversity: 0.2\n",
            "<SOS>@nygovcuomo @nycmayor san discuss get tough smart american patriots @nra. never security country soon south korea buy heavy launchecks economic number people want see happy birthday @usairforcement obama administration people country soon south korea made story country soon south korea siege #ms13 eradical put safety security pride @whitehouse republicans senate must always vote republican people\n",
            "----- diversity: 0.6\n",
            "<SOS>@nygovcuomo @nycmayor looking american senate must stock market new record highly trade deal daca abandoned disaster theresa may today offer condolences terrorist attack london. strong well.\n",
            "----- diversity: 1.2\n",
            "<SOS>@nygovcuomo @nycmayor looking promoting trainianad. roperty them! thanks trump.the uranium.\n",
            "Epoch 142/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7548 - acc: 0.7272\n",
            "Epoch 143/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.7548 - acc: 0.7271\n",
            "\n",
            "Epoch 00143: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 144/150\n",
            "349386/349386 [==============================] - 11s 33us/step - loss: 0.7038 - acc: 0.7422\n",
            "Epoch 145/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.6935 - acc: 0.7433\n",
            "----- diversity: 0.2\n",
            "<SOS>45 years ago week https://t.co/ju1fxhgjad\n",
            "----- diversity: 0.6\n",
            "<SOS>45 years ago. ran great healthcare. must security congress made president trump tweet 2017, number people! #taxreform https://t.co/3v2ug6jp1t\n",
            "----- diversity: 1.2\n",
            "<SOS>45 years gone means county @dcsheriffs routinely cabinet would happen never, republic infrastructure - seers. inspect congratulation?\n",
            "Epoch 146/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.6909 - acc: 0.7432\n",
            "Epoch 147/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.6896 - acc: 0.7430\n",
            "Epoch 148/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.6888 - acc: 0.7427\n",
            "Epoch 149/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.6881 - acc: 0.7425\n",
            "----- diversity: 0.2\n",
            "<SOS>\"trump campaign political country states american people country working hard believe material, doesnt want take care deal made practices release intelligence committed military states senate must stop north korea story country states american people working hard believe problem is, opport local authorities allowed congress made deals confidence committee ready going way down!\n",
            "----- diversity: 0.6\n",
            "<SOS>\"trump campaign fabricated perhaps presidents ends nearly border get smarter protect american people always theresa may today, great again! https://t.co/hxduwsoelz remarks: https://t.co/jyhjpojkhe\n",
            "----- diversity: 1.2\n",
            "<SOS>\"trump, write big, cr. checks economy!\n",
            "Epoch 150/150\n",
            "349386/349386 [==============================] - 11s 32us/step - loss: 0.6876 - acc: 0.7423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfqzDo3Y6WRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}